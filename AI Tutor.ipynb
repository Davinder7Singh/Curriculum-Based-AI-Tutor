{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Curriculum-Based AI Tutor"
      ],
      "metadata": {
        "id": "08h5FTBTJiGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "\n",
        "To develop an AI tutor that answers student questions using a Retrieval-Augmented Generation (RAG) approach.\n",
        "\n",
        "To enable semantic search over NCERT Class 8 Science content and provide accurate, grade-appropriate responses.\n",
        "\n",
        "To evaluate model outputs using BLEU, ROUGE-L, and human review, ensuring factual correctness and transparency with source citations.\n",
        "\n",
        "To deploy an interactive demo interface for students to engage with the AI tutor."
      ],
      "metadata": {
        "id": "BXkGH_E5Jn7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Dependencies for RAG-based AI Tutor"
      ],
      "metadata": {
        "id": "phCj6RU3E_-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyXU9MJ_4Wob",
        "outputId": "49cb30a7-2477-4fed-9613-0372ede50049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.11/dist-packages (1.7.4)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.31.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: huggingface_hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
            "Requirement already satisfied: transformers>=4.44.2 in /usr/local/lib/python3.11/dist-packages (4.55.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.2 in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: diffusers>=0.27.0 in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: gradio>=5.0.0 in /usr/local/lib/python3.11/dist-packages (5.42.0)\n",
            "Requirement already satisfied: peft>=0.10.0 in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.27.0) (1.1.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2) (0.6.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2) (11.3.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.27.0) (8.7.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (3.11.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.12.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=5.0.0) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio>=5.0.0) (15.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.10.0) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.10.0) (1.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=5.0.0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=5.0.0) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio>=5.0.0) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio>=5.0.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio>=5.0.0) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=5.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=5.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio>=5.0.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=5.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=5.0.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=5.0.0) (0.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.2) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=5.0.0) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=5.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=5.0.0) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers>=0.27.0) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.27.0) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.27.0) (2.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.2) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio>=5.0.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.0.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.0.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.0.0) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install faiss-cpu==1.7.4 pdfplumber nltk rouge-score groq\n",
        "!pip install numpy==1.26.4\n",
        "!pip install \"huggingface_hub>=0.27.0\" \"transformers>=4.44.2\" \"sentence-transformers>=2.2.2\" \"diffusers>=0.27.0\" \"gradio>=5.0.0\" \"peft>=0.10.0\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, Configuration, and Environment Setup"
      ],
      "metadata": {
        "id": "Ki0n0jr3Fbgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxu7dopu4dyx",
        "outputId": "8557944a-1fea-4a40-bb53-4cecc56e4afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: imports and config\n",
        "import os, json, csv, time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import pdfplumber\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# CONFIG - adjust as needed\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "# Either put a single PDF here OR a folder of PDFs (see extraction function)\n",
        "SINGLE_PDF_PATH = DATA_DIR / \"/content/science class 8.pdf\"          # path if you have one file\n",
        "#PDF_FOLDER = DATA_DIR / \"class8_pdfs\"                      # path if you have 13 separate pdfs (one per chapter)\n",
        "JSONL_PATH = DATA_DIR / \"class8_science.jsonl\"\n",
        "FAISS_INDEX_PATH = DATA_DIR / \"faiss_index.index\"\n",
        "EMB_MODEL_NAME = \"all-MiniLM-L6-v2\"    # all-MiniLM-L6-v2 shorthand (SF model)\n",
        "GROQ_MODEL = \"llama3-8b-8192\"             # default, change if needed and available to your key\n",
        "K_RETRIEVE = 5\n",
        "LOG_CSV = DATA_DIR / \"interaction_log.csv\"\n",
        "EVAL_CSV = DATA_DIR / \"evaluation.csv\"\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"gsk_g2Gu0krzPZOCvThl4MZ3WGdyb3FY9CQPJsxqFiNiuCA8i947EyX\"\n",
        "# Groq API key (must be set in env)\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
        "if not GROQ_API_KEY:\n",
        "    print(\"Warning: GROq_API_KEY not found in environment variables. \"\n",
        "          \"Set it before calling generation (e.g. in Colab: %env GROQ_API_KEY=sk-...)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-KGKcCw4puu"
      },
      "source": [
        "# PDF Text Extraction with Chapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOeZukQB4piS",
        "outputId": "c13598da-143d-4911-adb0-370582dc58a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting from single PDF: /content/science class 8.pdf\n",
            "Extracted 47 document chunks (candidate chapters/pages).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def extract_from_single_pdf(pdf_path: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extracts full text from one big PDF and attempts to detect chapter splits by heading keywords.\n",
        "    Returns list of dicts: {\"id\",\"chapter\",\"section\",\"text\"}.\n",
        "    \"\"\"\n",
        "    items = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        pages = [p.extract_text() or \"\" for p in pdf.pages]\n",
        "    text_all = \"\\n\".join(pages)\n",
        "    # Try split on common \"Chapter\" headings (case-insensitive)\n",
        "    parts = re.split(r'\\n\\s*(CHAPTER|Chapter)\\b.*', text_all)\n",
        "    if len(parts) <= 2:\n",
        "        # fallback: create page-level items\n",
        "        for i, p in enumerate(pages):\n",
        "            if p.strip():\n",
        "                items.append({\"id\": f\"page_{i}\", \"chapter\": f\"page_{i}\", \"section\": f\"page_{i}\", \"text\": p.strip()})\n",
        "    else:\n",
        "        # naive chunks (some PDFs have inconsistent headings; you'll likely want to review)\n",
        "        cur_id = 0\n",
        "        for chunk in parts:\n",
        "            txt = chunk.strip()\n",
        "            if not txt:\n",
        "                continue\n",
        "            items.append({\"id\": f\"chunk_{cur_id}\", \"chapter\": f\"chunk_{cur_id}\", \"section\": f\"chunk_{cur_id}\", \"text\": txt})\n",
        "            cur_id += 1\n",
        "    return items\n",
        "\n",
        "def extract_from_pdf_folder(folder: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extracts text from each PDF file in folder. Uses filename as chapter name.\n",
        "    \"\"\"\n",
        "    items = []\n",
        "    pdf_files = sorted(list(folder.glob(\"*.pdf\")))\n",
        "    for pdf_path in pdf_files:\n",
        "        chap_name = pdf_path.stem\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            pages = [p.extract_text() or \"\" for p in pdf.pages]\n",
        "        text_all = \"\\n\".join(pages).strip()\n",
        "        # optional: split by \"Chapter\" inside each file too\n",
        "        items.append({\"id\": chap_name, \"chapter\": chap_name, \"section\": chap_name, \"text\": text_all})\n",
        "    return items\n",
        "\n",
        "# Use whichever is present\n",
        "docs = []\n",
        "if SINGLE_PDF_PATH.exists():\n",
        "    print(\"Extracting from single PDF:\", SINGLE_PDF_PATH)\n",
        "    docs = extract_from_single_pdf(SINGLE_PDF_PATH)\n",
        "elif PDF_FOLDER.exists():\n",
        "    print(\"Extracting from PDF folder:\", PDF_FOLDER)\n",
        "    docs = extract_from_pdf_folder(PDF_FOLDER)\n",
        "else:\n",
        "    print(\"No PDF found. Upload 'class8_science.pdf' to data/ or put chapter PDFs into data/class8_pdfs/\")\n",
        "\n",
        "print(f\"Extracted {len(docs)} document chunks (candidate chapters/pages).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning and Chunking into Passages with Metadata"
      ],
      "metadata": {
        "id": "s3l72ZrIGf95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "31ff4100cf264be8834c5adcea049d4e",
            "247b3b020a174523b1d651ea66e64223",
            "2c3b5214da0a4bdd942ed016d7fce20f",
            "907be581c53c4bb7a33e598b3748eb0f",
            "7e9213bc4ac44b35ae49e637228cb498",
            "0c85d1ab1ee24f0786f31aa059dfb562",
            "d321ec8347fd4665baf938265626e050",
            "591b19977ad04b9eaa31a14078fb307a",
            "c4b3ddef02f44a43a2c87666c4bf46d3",
            "f6ded386dad3460eb390850b51277d74",
            "ca5141931f24444b803eb186b3438589"
          ]
        },
        "id": "WzkO9Q6kOZyy",
        "outputId": "00576ab8-f9b5-42a1-a8ce-9ca8dd9ad912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 594 passages.\n",
            "Saved cleaned corpus to data/class8_science.jsonl\n",
            "Loading embedding model: all-MiniLM-L6-v2\n",
            "Embedding 594 passages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31ff4100cf264be8834c5adcea049d4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built and saved to data/faiss_index.index\n",
            "Saved passage metadata to data/passage_metadata.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    # Basic cleaning for NCERT-like text\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Merge newlines\n",
        "    text = re.sub(r'\\b\\d+\\.\\s+[A-Za-z ]{1,20}', '', text)  # Remove short numbered clues\n",
        "    text = text.replace(\"1 C 2 O\", \"CO₂\").replace(\"C 2 O\", \"CO₂\")\n",
        "    text = re.sub(r'\\b[A-Z]\\b', '', text)  # Remove single uppercase letters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Collapse extra spaces\n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text_into_passages(text: str, max_tokens_or_words: int = 150) -> List[str]:\n",
        "    text = clean_text(text)\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    cur = []\n",
        "    cur_len = 0\n",
        "    chunks = []\n",
        "    for s in sents:\n",
        "        words = s.split()\n",
        "        if cur_len + len(words) <= max_tokens_or_words:\n",
        "            cur.append(s)\n",
        "            cur_len += len(words)\n",
        "        else:\n",
        "            chunks.append(\" \".join(cur).strip())\n",
        "            cur = [s]\n",
        "            cur_len = len(words)\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur).strip())\n",
        "    chunks = [c for c in chunks if len(c) > 20]  # Filter very short chunks\n",
        "    return chunks\n",
        "\n",
        "# Build passage list with metadata\n",
        "passages = []\n",
        "passage_id = 0\n",
        "for d in docs:\n",
        "    text = d.get(\"text\", \"\")\n",
        "    chapter = d.get(\"chapter\", \"unknown\")\n",
        "    chunks = chunk_text_into_passages(text, max_tokens_or_words=140)\n",
        "    for i, c in enumerate(chunks):\n",
        "        passages.append({\n",
        "            \"id\": f\"p_{passage_id}\",\n",
        "            \"chapter\": chapter,\n",
        "            \"section\": d.get(\"section\", \"\"),\n",
        "            \"para_index\": i,\n",
        "            \"text\": c\n",
        "        })\n",
        "        passage_id += 1\n",
        "\n",
        "print(\"Created\", len(passages), \"passages.\")\n",
        "with open(JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in passages:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "print(\"Saved cleaned corpus to\", JSONL_PATH)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Passages and Building FAISS Index"
      ],
      "metadata": {
        "id": "CVI8pNUkjRcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"Loading embedding model:\", EMB_MODEL_NAME)\n",
        "embed_model = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "def embed_texts(model: SentenceTransformer, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
        "    return np.array(\n",
        "        model.encode(texts, show_progress_bar=True, convert_to_numpy=True, batch_size=batch_size),\n",
        "        dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "texts = [p[\"text\"] for p in passages]\n",
        "print(\"Embedding\", len(texts), \"passages...\")\n",
        "embeddings = embed_texts(embed_model, texts)\n",
        "\n",
        "# Build FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "faiss.write_index(index, str(FAISS_INDEX_PATH))\n",
        "print(\"FAISS index built and saved to\", FAISS_INDEX_PATH)\n",
        "\n",
        "# Save metadata mapping\n",
        "METADATA_PATH = DATA_DIR / \"passage_metadata.json\"\n",
        "with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(passages, f, ensure_ascii=False, indent=2)\n",
        "print(\"Saved passage metadata to\", METADATA_PATH)\n",
        "\n"
      ],
      "metadata": {
        "id": "ubw7sBaiGEk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verifying Cleaned Corpus Data"
      ],
      "metadata": {
        "id": "dbmx2KpzjaTD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drgU0qUbMM6q",
        "outputId": "5a6af8a2-bfbf-48fb-cb0c-cd6dffc5de3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previewing first 3 passages from data/class8_science.jsonl\n",
            "{'id': 'p_0', 'chapter': 'chunk_0', 'section': 'chunk_0', 'para_index': 0, 'text': 'SSCCIIEENNCCEE VIII EXTBOOK FOR LASS 2018-19 ISBN 978-81-7450-812-6 First Edition January 2008 Magha 1929 ALL RIGHTS RESERVED Reprint Edition q No part of this publication may be reproduced, stored in a retrieval December 2008 Pausa 1930 system or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise without the prior January 2010 Magha 1931 permission of the publisher. November 2010 Kartika 1932 q This book is sold subject to the condition that it shall not, by way of January 2012 Magha 1933 trade, be lent, re-sold, hired out or otherwise disposed of without the publisher’s consent, in any form of binding or cover other than that in November 2012 Kartika 1934 which it is published.'}\n",
            "{'id': 'p_1', 'chapter': 'chunk_0', 'section': 'chunk_0', 'para_index': 1, 'text': 'October 2013 Asvina 1935 q The correct price of this publication is the price printed on this page, December 2014 Pausa 1936 Any revised price indicated by a rubber stamp or by a sticker or by any other means is incorrect and should be unacceptable. December 2015 Agrahayana 1937 February 2017 Magha 1938 December 2017 Agrahayana 1939 OFFICES OF THE PUBLICATION PD 750T+100T RPS DIVISION,, NCERT NCERT Campus © National Council of Educational Sri Aurobindo Marg New Delhi 110 016 Phone : 011-26562708 Research and Training, 2008 108, 100 Feet Road Hosdakere Halli Extension Banashankari III Stage Bangaluru 560 085 Phone : 080-26725740 Navjivan Trust Building ..Navjivan Ahmedabad 380 014 Phone : 079-27541446 CWC Campus Opp.'}\n",
            "{'id': 'p_2', 'chapter': 'chunk_0', 'section': 'chunk_0', 'para_index': 2, 'text': 'Dhankal Bus Stop Panihati Kolkata 700 114 Phone : 033-25530454 CWC Complex Maligaon Guwahati 781 021 Phone : 0361-2674869 ` 55.00 Publication Team Head, Publication : . Siraj Anwar Division Chief Editor : Shveta Uppal Chief Business : Gautam Ganguly Manager Chief Production : Arun Chitkara Officer (In-charge) Printed on 80 GSM paper with NCERT watermark Assistant Editor : Shashi Chadha Published at the Publication Division, Production Assistant : Om Prakash by the Secretary, National Council of Educational Research and Training, Sri Aurobindo Marg, New Delhi 110 016 and Cover, Layout and Illustrations printed at Educational Stores, -5, Ashwani Tyagi Bulandshahar Road, Industrial Area Site- (Near RTO Office) Ghaziabad (..) 2018-19 Foreword The National Curriculum Framework (NCF), 2005, recommends that children’s life at school must be linked to their life outside the school.'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "print(\"Previewing first 3 passages from\", JSONL_PATH)\n",
        "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 3:\n",
        "            break\n",
        "        print(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Bxzrgx6PYH"
      },
      "source": [
        "# Retrieval function (get top-k passages and return them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H42Cle156TZU",
        "outputId": "c0422167-e530-4b3a-c5a7-754c56f9983b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 5 snippets. Example snippet:\n",
            " These carry genes and a nuclear membrane are designated help in inheritance or transfer of as eukaryotic cells. All organisms other than bacteria and blue characters from the parents to the green algae are called eukaryotes. offspring. The chromosomes can be seen (eu : true; karyon: nucleus). only when the cell divides. CELL — STRUCTURE AND FUNCTIONS 95 2018-19 While observing the onion cells call\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def load_index_and_meta(index_path: str, meta_path: str):\n",
        "    idx = faiss.read_index(index_path)\n",
        "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        meta = json.load(f)\n",
        "    return idx, meta\n",
        "\n",
        "def retrieve(query: str, top_k: int = 5) -> List[Dict]:\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    idx = faiss.read_index(str(FAISS_INDEX_PATH))\n",
        "    _, I = idx.search(np.array(q_emb, dtype=\"float32\"), top_k)\n",
        "    hits = []\n",
        "    for ii in I[0]:\n",
        "        if ii < 0 or ii >= len(passages):\n",
        "            continue\n",
        "        hits.append(passages[ii])\n",
        "    return hits\n",
        "\n",
        "# Quick test\n",
        "q = \"What is photosynthesis?\"\n",
        "hits = retrieve(q, top_k=5)\n",
        "print(\"Retrieved\", len(hits), \"snippets. Example snippet:\\n\", hits[0][\"text\"][:400])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RDsnPu6Mvyp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWciMJn66ZV_"
      },
      "source": [
        "# Groq generation wrapper (uses only GROQ_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_0JH9AH6ZHp",
        "outputId": "f2044d9a-47d1-4b64-b38e-3fd2a2251a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n",
        "\n",
        "try:\n",
        "    from groq import Client as GroqClient\n",
        "    groq_client = GroqClient(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "    def groq_generate(prompt: str, model: str = GROQ_MODEL, max_tokens: int = 512) -> str:\n",
        "\n",
        "        if hasattr(groq_client, \"chat\"):\n",
        "            resp = groq_client.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":prompt}], max_tokens=max_tokens)\n",
        "            # try to parse likely shapes:\n",
        "            try:\n",
        "                return resp.choices[0].message.content\n",
        "            except Exception:\n",
        "                # older shape\n",
        "                return str(resp)\n",
        "        elif hasattr(groq_client, \"text\"):\n",
        "            resp = groq_client.text.create(model=model, input=prompt, max_tokens=max_tokens)\n",
        "            try:\n",
        "                # typical dict shape\n",
        "                return resp[\"output\"][0][\"content\"][0][\"text\"]\n",
        "            except Exception:\n",
        "                return str(resp)\n",
        "        else:\n",
        "            # fallback\n",
        "            return str(groq_client.create(prompt))\n",
        "except Exception as e:\n",
        "    print(\"Groq client import error or not installed. Generation will fail until groq package available. Error:\", e)\n",
        "    def groq_generate(prompt: str, model: str = GROQ_MODEL, max_tokens: int = 512) -> str:\n",
        "        raise RuntimeError(\"Groq client not available. Install 'groq' and set GROQ_API_KEY in env.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Construction and Question Answering Function"
      ],
      "metadata": {
        "id": "-jfuAI1lHvaw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i69dv3ws6hit"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_prompt(question: str, retrieved: List[Dict]) -> str:\n",
        "    snippets = []\n",
        "    for i, s in enumerate(retrieved):\n",
        "        snippets.append(f\"[{i+1}] (Chapter: {s.get('chapter')})\\n{s.get('text')}\\n\")\n",
        "    context_block = \"\\n\\n\".join(snippets)\n",
        "    prompt = f\"\"\"\n",
        "You are an AI tutor for NCERT Class 8 Science. Answer the student's question using ONLY the facts from the provided textbook snippets below.\n",
        "If the question cannot be answered from the textbook content, reply exactly:\n",
        "\n",
        "\"I'm focused on Class 8 Science content. I don't have a textbook answer for that. Try re-phrasing your question to be within the textbook.\"\n",
        "\n",
        "Provide:\n",
        "1) A short, grade-appropriate answer (1–6 sentences).\n",
        "2) A 'Sources' line listing the snippet indices you used, e.g. [1],[3].\n",
        "\n",
        "Context snippets:\n",
        "{context_block}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def answer_question(question: str, top_k: int = K_RETRIEVE) -> Dict:\n",
        "    retrieved = retrieve(question, top_k=top_k)\n",
        "    if not retrieved:\n",
        "        return {\"answer\":\"I'm focused on Class 8 Science; I don't have content to answer that question.\", \"retrieved\": []}\n",
        "    prompt = build_prompt(question, retrieved)\n",
        "    gen = groq_generate(prompt, model=GROQ_MODEL, max_tokens=512)\n",
        "    # Logging\n",
        "    with open(LOG_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([time.time(), question, gen, \"|\".join([r[\"id\"] for r in retrieved])])\n",
        "    return {\"answer\": gen, \"retrieved\": retrieved}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nbO88Or6sQN"
      },
      "source": [
        "# QA System Evaluation with BLEU, ROUGE-L & BERTScore\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score\n",
        "\n",
        "# Evaluation (with normalization + smoothing + multiple golds + source cleaning)\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bert_score import score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, remove punctuation/stopwords, lemmatize\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)   # keep only alphanum + spaces\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def clean_generated(gen: str) -> str:\n",
        "    \"\"\"Remove trailing sources or irrelevant parts from generated answer\"\"\"\n",
        "    if \"Sources:\" in gen:\n",
        "        gen = gen.split(\"Sources:\")[0]\n",
        "    return gen.strip()\n",
        "\n",
        "def evaluate(test_pairs: List[Dict], top_k: int = K_RETRIEVE) -> List[Dict]:\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rows = []\n",
        "\n",
        "    for t in test_pairs:\n",
        "        q = t[\"question\"]\n",
        "        golds = t[\"gold_answer\"]\n",
        "\n",
        "        # allow both single-string and multi-reference\n",
        "        if isinstance(golds, str):\n",
        "            golds = [golds]\n",
        "\n",
        "        out = answer_question(q, top_k=top_k)\n",
        "        gen = clean_generated(out[\"answer\"])\n",
        "\n",
        "        # normalized\n",
        "        gen_norm = normalize_text(gen)\n",
        "        golds_norm = [normalize_text(g) for g in golds]\n",
        "\n",
        "        # BLEU with smoothing\n",
        "        try:\n",
        "            bleu = sentence_bleu(\n",
        "                [nltk.word_tokenize(g) for g in golds_norm],\n",
        "                nltk.word_tokenize(gen_norm),\n",
        "                smoothing_function=smooth\n",
        "            )\n",
        "        except Exception:\n",
        "            bleu = 0.0\n",
        "\n",
        "        # ROUGE-L (take best among multiple golds)\n",
        "        try:\n",
        "            rougeL = max([scorer.score(g, gen_norm)[\"rougeL\"].fmeasure for g in golds_norm])\n",
        "        except Exception:\n",
        "            rougeL = 0.0\n",
        "\n",
        "        # BERTScore (semantic similarity, pick best gold)\n",
        "        try:\n",
        "            bert_f1 = max([score([gen], [g], lang=\"en\", verbose=False)[2].item() for g in golds])\n",
        "        except Exception:\n",
        "            bert_f1 = 0.0\n",
        "\n",
        "        rows.append({\n",
        "            \"question\": q,\n",
        "            \"gold\": golds[0] if len(golds)==1 else golds,  # keep first or list\n",
        "            \"gen\": gen,\n",
        "            \"bleu\": bleu,\n",
        "            \"rougeL\": rougeL,\n",
        "            \"bert_f1\": bert_f1\n",
        "        })\n",
        "\n",
        "    # Save\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(EVAL_CSV, index=False)\n",
        "    print(\"Saved evaluation to\", EVAL_CSV)\n",
        "\n",
        "    return rows\n",
        "test_pairs = [\n",
        "    {\n",
        "        \"question\": \"What is crop rotation?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Crop rotation is the practice of growing different crops in succession on the same field to maintain soil fertility.\",\n",
        "            \"Crop rotation means planting different crops alternately in the same field to prevent soil nutrients from being exhausted.\",\n",
        "            \"It is the method of cultivating different crops one after another on the same land to keep the soil fertile.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Name two methods of irrigation used in agriculture.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Two common methods of irrigation are sprinkler system and drip irrigation.\",\n",
        "            \"Irrigation can be done by sprinkler method or drip method.\",\n",
        "            \"Two important irrigation techniques are the sprinkler and the drip system.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are microorganisms?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Microorganisms are tiny living organisms, such as bacteria, fungi, protozoa and some algae, which can only be seen under a microscope.\",\n",
        "            \"Very small organisms like bacteria, protozoa, some fungi and algae that cannot be seen without a microscope are called microorganisms.\",\n",
        "            \"Microorganisms are minute organisms that are visible only under a microscope.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Name one disease caused by bacteria and one caused by protozoa.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Tuberculosis is caused by bacteria, while malaria is caused by protozoa.\",\n",
        "            \"A bacterial disease is tuberculosis, and a protozoan disease is malaria.\",\n",
        "            \"Tuberculosis is an example of a bacterial disease; malaria is an example of a protozoan disease.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are synthetic fibres? Give two examples.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Synthetic fibres are man-made fibres prepared from chemicals. Examples include nylon and polyester.\",\n",
        "            \"Fibres made by humans from chemicals are called synthetic fibres, such as nylon and polyester.\",\n",
        "            \"Synthetic fibres are artificial fibres, for example polyester and nylon.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Write one advantage and one disadvantage of plastics.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Plastics are durable and light in weight, but they are non-biodegradable and cause pollution.\",\n",
        "            \"Plastics are strong and light, but they do not decompose easily and pollute the environment.\",\n",
        "            \"An advantage of plastic is durability; a disadvantage is that it is non-biodegradable.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Name two metals and two non-metals.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Examples of metals are iron and copper, while examples of non-metals are sulphur and carbon.\",\n",
        "            \"Iron and copper are metals; sulphur and carbon are non-metals.\",\n",
        "            \"Two metals are iron and copper; two non-metals are carbon and sulphur.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are fossil fuels?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Fossil fuels are natural fuels like coal, petroleum, and natural gas formed from the remains of dead plants and animals over millions of years.\",\n",
        "            \"Coal, petroleum, and natural gas are fossil fuels formed from ancient plant and animal remains.\",\n",
        "            \"Fuels such as coal, oil, and gas formed from buried dead organisms are called fossil fuels.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Why is petroleum called black gold?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Petroleum is called black gold because of its black colour and high economic value.\",\n",
        "            \"Due to its dark colour and great commercial importance, petroleum is known as black gold.\",\n",
        "            \"Petroleum is referred to as black gold since it is black in colour and extremely valuable.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is combustion?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Combustion is a chemical process in which a substance reacts with oxygen to release heat and light.\",\n",
        "            \"The burning of a substance with oxygen to give out heat and light is called combustion.\",\n",
        "            \"Combustion is the process of burning in which a material reacts with oxygen producing heat and light.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the SI unit of force?\",\n",
        "        \"gold_answer\": [\n",
        "            \"The SI unit of force is newton (N).\",\n",
        "            \"Force is measured in newtons in the SI system.\",\n",
        "            \"The standard SI unit for force is called a newton.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Define pressure.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Pressure is the force acting per unit area.\",\n",
        "            \"When a force is applied per unit area, it is called pressure.\",\n",
        "            \"Pressure is defined as force divided by area.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the force of friction?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Friction is the opposing force that comes into play when one surface moves or tends to move over another surface.\",\n",
        "            \"The contact force that resists motion between two surfaces is called friction.\",\n",
        "            \"Friction is the force that opposes relative motion between two surfaces in contact.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is a cell?\",\n",
        "        \"gold_answer\": [\n",
        "            \"A cell is the structural and functional unit of life.\",\n",
        "            \"The basic structural and functional unit of all living organisms is called a cell.\",\n",
        "            \"Cells are the smallest units of life, making up the structure and function of organisms.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the function of haemoglobin?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Haemoglobin in red blood cells helps transport oxygen from the lungs to body tissues.\",\n",
        "            \"The function of haemoglobin is to carry oxygen in the blood.\",\n",
        "            \"Haemoglobin transports oxygen from the lungs to all body parts.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is photosynthesis?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Photosynthesis is the process by which green plants prepare their own food from carbon dioxide and water using sunlight in the presence of chlorophyll.\",\n",
        "            \"Green plants make food by using water and carbon dioxide in sunlight; this is called photosynthesis.\",\n",
        "            \"Photosynthesis is the process where plants prepare food in the presence of chlorophyll and sunlight.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Name two methods of water purification used at home.\",\n",
        "        \"gold_answer\": [\n",
        "            \"Two methods of water purification are boiling and filtration.\",\n",
        "            \"Water can be purified at home by boiling or by filtering.\",\n",
        "            \"Boiling and filtration are common methods to make water safe for drinking at home.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is pollution?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Pollution is the undesirable change in the physical, chemical, or biological characteristics of air, water, or soil.\",\n",
        "            \"When air, water, or soil is contaminated, it is called pollution.\",\n",
        "            \"Pollution means harmful changes in the environment caused by impurities.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the ozone layer?\",\n",
        "        \"gold_answer\": [\n",
        "            \"The ozone layer protects the Earth from harmful ultraviolet rays of the Sun.\",\n",
        "            \"A layer of ozone gas in the atmosphere that blocks harmful UV radiation is called the ozone layer.\",\n",
        "            \"The ozone layer shields Earth by absorbing ultraviolet rays from the Sun.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is global warming?\",\n",
        "        \"gold_answer\": [\n",
        "            \"Global warming is the gradual increase in the Earth’s average temperature due to excessive greenhouse gases in the atmosphere.\",\n",
        "            \"The rise in Earth’s average temperature caused by greenhouse gases is known as global warming.\",\n",
        "            \"Global warming means the slow increase in Earth’s temperature due to carbon dioxide and other gases.\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "#  Run evaluation\n",
        "rows = evaluate(test_pairs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rntlCGdJtDR9",
        "outputId": "31759c70-642b-4a2a-af40-08f731d234f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.55.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.34.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.8.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved evaluation to data/evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show saved files"
      ],
      "metadata": {
        "id": "CCa-piqZIh8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW4hr9xz7Kby",
        "outputId": "abdc3d90-b5cd-4725-cddb-10918057cce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files created in data/:\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: show saved files and next steps\n",
        "print(\"Files created in data/:\")\n",
        "for p in DATA_DIR.iterdir():\n",
        "    print(\"-\", p.name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31ff4100cf264be8834c5adcea049d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_247b3b020a174523b1d651ea66e64223",
              "IPY_MODEL_2c3b5214da0a4bdd942ed016d7fce20f",
              "IPY_MODEL_907be581c53c4bb7a33e598b3748eb0f"
            ],
            "layout": "IPY_MODEL_7e9213bc4ac44b35ae49e637228cb498"
          }
        },
        "247b3b020a174523b1d651ea66e64223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c85d1ab1ee24f0786f31aa059dfb562",
            "placeholder": "​",
            "style": "IPY_MODEL_d321ec8347fd4665baf938265626e050",
            "value": "Batches: 100%"
          }
        },
        "2c3b5214da0a4bdd942ed016d7fce20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_591b19977ad04b9eaa31a14078fb307a",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4b3ddef02f44a43a2c87666c4bf46d3",
            "value": 10
          }
        },
        "907be581c53c4bb7a33e598b3748eb0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ded386dad3460eb390850b51277d74",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5141931f24444b803eb186b3438589",
            "value": " 10/10 [00:58&lt;00:00,  4.25s/it]"
          }
        },
        "7e9213bc4ac44b35ae49e637228cb498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c85d1ab1ee24f0786f31aa059dfb562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d321ec8347fd4665baf938265626e050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "591b19977ad04b9eaa31a14078fb307a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b3ddef02f44a43a2c87666c4bf46d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6ded386dad3460eb390850b51277d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5141931f24444b803eb186b3438589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}